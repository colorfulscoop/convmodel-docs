{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"convmodel convmodel provides a conversation model based on transformers GPT-2 model Features Utilizes GPT2 model to generate response Handles multi-turn conversation Provides useuful interfaces to fine-tune model and generate a response from a given context A simple example of fine-tune GPT-2 model and generate a response: from convmodel import ConversationModel from convmodel import ConversationExample # Load model on GPU model = ConversationModel . from_pretrained ( \"gpt2\" ) # Define training/validation examples train_iterator = [ ConversationExample ( conversation = [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ]), ConversationExample ( conversation = [ \"I am hungry\" , \"How about eating pizza?\" ]), ] valid_iterator = [ ConversationExample ( conversation = [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ]), ] # Fine-tune model model . fit ( train_iterator = train_iterator , valid_iterator = valid_iterator ) # Generate response model . generate ( context = [ \"Hello\" , \"How are you\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) # Output could be like below if sufficient examples were given. # => ConversationModelOutput(responses=['Good thank you'], context=['Hello', 'How are you']) Please refer to Model Training for more details of training. ConversationModel adopts simple input schema by concatenating each utterance with <sep> token as below. Please refer to Model Architecture Overview for more details. position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep> Enjoy talking with your conversational AI","title":"Overview"},{"location":"#convmodel","text":"convmodel provides a conversation model based on transformers GPT-2 model Features Utilizes GPT2 model to generate response Handles multi-turn conversation Provides useuful interfaces to fine-tune model and generate a response from a given context A simple example of fine-tune GPT-2 model and generate a response: from convmodel import ConversationModel from convmodel import ConversationExample # Load model on GPU model = ConversationModel . from_pretrained ( \"gpt2\" ) # Define training/validation examples train_iterator = [ ConversationExample ( conversation = [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ]), ConversationExample ( conversation = [ \"I am hungry\" , \"How about eating pizza?\" ]), ] valid_iterator = [ ConversationExample ( conversation = [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ]), ] # Fine-tune model model . fit ( train_iterator = train_iterator , valid_iterator = valid_iterator ) # Generate response model . generate ( context = [ \"Hello\" , \"How are you\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) # Output could be like below if sufficient examples were given. # => ConversationModelOutput(responses=['Good thank you'], context=['Hello', 'How are you']) Please refer to Model Training for more details of training. ConversationModel adopts simple input schema by concatenating each utterance with <sep> token as below. Please refer to Model Architecture Overview for more details. position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep> Enjoy talking with your conversational AI","title":"convmodel"},{"location":"cli/","text":"CLI (Experimental) Warning Currently convmodel CLI is an experimental feature. CLI provides commands to continue \"fit - eval - try\" loop to improve your conversation model. To use convmodel CLI, install convmodel with cli option. $ pip install convmodel [ cli ] fit - train your model This is a simple wrapper interface of ConversationModel.fit method. You can simply run training based on a json config file. All you need to do is preparing json config file. You can generate a template file as follows. $ python -m convmodel fit --print_config >config.json $ cat config.json { \"pretrained_model_or_path\" : \"\" , \"output_path\" : \"\" , \"train_file\" : \"\" , \"valid_file\" : \"\" , \"eval_file\" : null, \"save_best_model\" : false, \"device\" : null, \"lr\" : 0 .0001, \"warmup_steps\" : 10000 , \"use_amp\" : false, \"epochs\" : 1 , \"accumulation_steps\" : 1 , \"show_progress_bar\" : true, \"log_steps\" : 100 , \"shuffle_buffer_size\" : null, \"batch_size\" : 1 , \"num_workers\" : 0 , \"prefetch_factor\" : 2 , \"seed\" : null, \"deterministic\" : false } At least you need to edit 4 parameters. Parameter Description Example value pretrained_model_or_path Pretrained model path to use gpt2 output_path Path to save your trained model model train_file Path for training data file. The format should be Json Lines. Each line needs to contain a list of string, which are one example of conversation input/train.jsonl valid_file Path for validation data file. Format is the same as train_file . input/valid.jsonl The format of train and valid files should be JSON Lines . Each line should be a list of utterances of each conversation. One example of the files is as follows. $ head -n3 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] After preparing config json file, you can start training by fit CLI command. $ python -m convmodel fit --config config.json Once training completes, you can load the trained model from output_path for ConversationModel . >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) eval - evaluate your model You can evaluate your model towards eval_file defined in config file. $ python -m convmodel eval --config config.json try - try your model convmodel CLI provides streamlit interface to test conversation of your model. # Default server address and port will be used $ python -m convmodel try # You can set server port via --server.port option $ python -m convmodel try --server.port 8080 # You can set server address and port via --server.address $ python -m convmodel try --server.port 8080 --server.address 0 .0.0.0 # You can check all options by --help $ python -m convmodel try --help As default, you can access UI via http://localhost:8501/ .","title":"CLI (Experimental)"},{"location":"cli/#cli-experimental","text":"Warning Currently convmodel CLI is an experimental feature. CLI provides commands to continue \"fit - eval - try\" loop to improve your conversation model. To use convmodel CLI, install convmodel with cli option. $ pip install convmodel [ cli ]","title":"CLI (Experimental)"},{"location":"cli/#fit-train-your-model","text":"This is a simple wrapper interface of ConversationModel.fit method. You can simply run training based on a json config file. All you need to do is preparing json config file. You can generate a template file as follows. $ python -m convmodel fit --print_config >config.json $ cat config.json { \"pretrained_model_or_path\" : \"\" , \"output_path\" : \"\" , \"train_file\" : \"\" , \"valid_file\" : \"\" , \"eval_file\" : null, \"save_best_model\" : false, \"device\" : null, \"lr\" : 0 .0001, \"warmup_steps\" : 10000 , \"use_amp\" : false, \"epochs\" : 1 , \"accumulation_steps\" : 1 , \"show_progress_bar\" : true, \"log_steps\" : 100 , \"shuffle_buffer_size\" : null, \"batch_size\" : 1 , \"num_workers\" : 0 , \"prefetch_factor\" : 2 , \"seed\" : null, \"deterministic\" : false } At least you need to edit 4 parameters. Parameter Description Example value pretrained_model_or_path Pretrained model path to use gpt2 output_path Path to save your trained model model train_file Path for training data file. The format should be Json Lines. Each line needs to contain a list of string, which are one example of conversation input/train.jsonl valid_file Path for validation data file. Format is the same as train_file . input/valid.jsonl The format of train and valid files should be JSON Lines . Each line should be a list of utterances of each conversation. One example of the files is as follows. $ head -n3 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] After preparing config json file, you can start training by fit CLI command. $ python -m convmodel fit --config config.json Once training completes, you can load the trained model from output_path for ConversationModel . >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" )","title":"fit - train your model"},{"location":"cli/#eval-evaluate-your-model","text":"You can evaluate your model towards eval_file defined in config file. $ python -m convmodel eval --config config.json","title":"eval - evaluate your model"},{"location":"cli/#try-try-your-model","text":"convmodel CLI provides streamlit interface to test conversation of your model. # Default server address and port will be used $ python -m convmodel try # You can set server port via --server.port option $ python -m convmodel try --server.port 8080 # You can set server address and port via --server.address $ python -m convmodel try --server.port 8080 --server.address 0 .0.0.0 # You can check all options by --help $ python -m convmodel try --help As default, you can access UI via http://localhost:8501/ .","title":"try - try your model"},{"location":"install/","text":"Install Following steps are required to complete installing convmodel. Prepare Python3.8+ environment Install PyTorch Install convmodel Prepare Python 3.8+ First, prepare Python 3.8+ environment. Install PyTorch Then install PyTorch >= 1.8,<=1.9. Please refer to official document to find out correct installation for your environment. Some examples of installtion are as follows. Install in Docker container without GPU $ docker container run -w /work -v $( pwd ) :/work --rm -it python:3.8.6-slim-buster bash ( container ) $ pip install torch == 1 .8.1 Install in Docker container enabling GPU and CUDA 11.1 Assume that CUDA 11.1 is installed in your environment. $ docker container run --gpus all --ipc = host --rm -it -v $( pwd ) :/work -w /work nvidia/cuda:11.1-devel-ubuntu20.04 bash --ipc option is required because share memory would not be enough because DataLoader multiprocess requires them. Refer to the pytorch discussion for more details. Then install Python3. ( container ) $ apt update && apt install -y python3 python3-pip git Install PyTorch which corresponds to your environment by following the installation guide . For example, in CUDA 11.1 environment, PyTorch can be installed as follows. ( container ) $ pip3 install torch == 1 .8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html Install convmodel Finally, install convmodel from PyPI. $ pip install convmodel If you want to run tests, specify [test] option to install dependencies. $ pip install convmodel [ test ]","title":"Install"},{"location":"install/#install","text":"Following steps are required to complete installing convmodel. Prepare Python3.8+ environment Install PyTorch Install convmodel","title":"Install"},{"location":"install/#prepare-python-38","text":"First, prepare Python 3.8+ environment.","title":"Prepare Python 3.8+"},{"location":"install/#install-pytorch","text":"Then install PyTorch >= 1.8,<=1.9. Please refer to official document to find out correct installation for your environment. Some examples of installtion are as follows. Install in Docker container without GPU $ docker container run -w /work -v $( pwd ) :/work --rm -it python:3.8.6-slim-buster bash ( container ) $ pip install torch == 1 .8.1 Install in Docker container enabling GPU and CUDA 11.1 Assume that CUDA 11.1 is installed in your environment. $ docker container run --gpus all --ipc = host --rm -it -v $( pwd ) :/work -w /work nvidia/cuda:11.1-devel-ubuntu20.04 bash --ipc option is required because share memory would not be enough because DataLoader multiprocess requires them. Refer to the pytorch discussion for more details. Then install Python3. ( container ) $ apt update && apt install -y python3 python3-pip git Install PyTorch which corresponds to your environment by following the installation guide . For example, in CUDA 11.1 environment, PyTorch can be installed as follows. ( container ) $ pip3 install torch == 1 .8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html","title":"Install PyTorch"},{"location":"install/#install-convmodel","text":"Finally, install convmodel from PyPI. $ pip install convmodel If you want to run tests, specify [test] option to install dependencies. $ pip install convmodel [ test ]","title":"Install convmodel"},{"location":"model_architecture_overview/","text":"Model Architecture Overview convmodel provides ConversationModel class. ConversationModel class adopts GPT2LMHeadModel architecture provided by transformers library. Although, in a initializer of ConversationModel , ConversationTokenizer is automatically initialized, let us first directly initialize ConversationTokenizer to see it encodes a given context to input to the model. Assume that ConversationTokenizer gets a context [\"Hello\", \"How are you\"] . Then ConversationTokenizer encodes it as follows. >>> from convmodel import ConversationTokenizer >>> tokenizer = ConversationTokenizer . from_pretrained ( \"gpt2\" ) >>> context = [ \"Hello\" , \"How are you\" ] >>> tokenizer ( context ) { 'input_ids' : [ 50256 , 15496 , 50256 , 2437 , 389 , 345 , 50256 ], 'token_type_ids' : [ 0 , 0 , 1 , 1 , 1 , 1 , 0 ], 'attention_mask' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 ]} position 0 1 2 3 4 5 6 word \\<sep> Hello \\<sep> How are you \\<sep> input_ids 50256 15496 50256 2437 389 345 50256 token_type_ids 0 0 1 1 1 1 0 attention_mask 1 1 1 1 1 1 1 Note: if a tokenizer does not assign a value to sep_token_id , it is automatically set with sep_token of <sep> . When initializing ConversationModel , ConversationTokenizer is automatically initialized inside. ConversationModel implements generate method. In generate method, an input context is first encoded as above. Then the encoded tensors are forwardded by the model to predict following tokens until <sep> token appears Note: Here we assume that model directory contains a trained conversation model which was fine-tuned from gpt2 model. We will see how to train our own conversation model later. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) >>> model . generate ( context , do_sample = True , top_p = 0.95 ) position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep>","title":"Model Architecture"},{"location":"model_architecture_overview/#model-architecture-overview","text":"convmodel provides ConversationModel class. ConversationModel class adopts GPT2LMHeadModel architecture provided by transformers library. Although, in a initializer of ConversationModel , ConversationTokenizer is automatically initialized, let us first directly initialize ConversationTokenizer to see it encodes a given context to input to the model. Assume that ConversationTokenizer gets a context [\"Hello\", \"How are you\"] . Then ConversationTokenizer encodes it as follows. >>> from convmodel import ConversationTokenizer >>> tokenizer = ConversationTokenizer . from_pretrained ( \"gpt2\" ) >>> context = [ \"Hello\" , \"How are you\" ] >>> tokenizer ( context ) { 'input_ids' : [ 50256 , 15496 , 50256 , 2437 , 389 , 345 , 50256 ], 'token_type_ids' : [ 0 , 0 , 1 , 1 , 1 , 1 , 0 ], 'attention_mask' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 ]} position 0 1 2 3 4 5 6 word \\<sep> Hello \\<sep> How are you \\<sep> input_ids 50256 15496 50256 2437 389 345 50256 token_type_ids 0 0 1 1 1 1 0 attention_mask 1 1 1 1 1 1 1 Note: if a tokenizer does not assign a value to sep_token_id , it is automatically set with sep_token of <sep> . When initializing ConversationModel , ConversationTokenizer is automatically initialized inside. ConversationModel implements generate method. In generate method, an input context is first encoded as above. Then the encoded tensors are forwardded by the model to predict following tokens until <sep> token appears Note: Here we assume that model directory contains a trained conversation model which was fine-tuned from gpt2 model. We will see how to train our own conversation model later. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) >>> model . generate ( context , do_sample = True , top_p = 0.95 ) position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep>","title":"Model Architecture Overview"},{"location":"model_training/","text":"Model Training Prepare model First you need to load GPT-2 pretrained model. The model is easily loaded by using from_pretrained method defined in ConversationModel . from convmodel import ConversationModel model = ConverstationModel . from_pretrained ( \"gpt2\" ) If you want to use GPU to train, device option is for that. # Load model in GPU model = ConverstationModel . from_pretrained ( \"gpt2\" , device = \"cuda\" ) # Load model in CPU model = ConverstationModel . from_pretrained ( \"gpt2\" , device = \"cpu\" ) If you do not specify any values to device , GPU is used if available. Training data Before training, you also need to prepare training data. convmodel provides ConversationExample class which shows one example of conversation to use in training. You need to prepare iterator objects for train/valid data to provide one ConversationExample object in each step in the loop. from convmodel import ConversationExample train_iterator = [ ConversationExample ( conversation = [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ]), ConversationExample ( conversation = [ \"I am hungry\" , \"How about eating pizza?\" ]), ] valid_iterator = [ ConvesationExample ( conversation = [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ]), ] Although the above example is fine, the data is usually large and difficult to load all the data on memory at the same time. In this case, it might be better to implement iterator class to provide one example in each step in the loop. Following example assumes each data file contains one conversation example in one line. The file format is Json Lines and each line contains a list of string which shows one conversation examples. # Assume that training/valid data is located in under input directory. # Training file: input/train.jsonl $ head -n2 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] # Validation file: input/valid.jsonl $ head -n1 input/valid.jsonl [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] You can implement your own iterator class to load the file and return each conversation example at each time as follows. class JsonLinesIterator : \"\"\"Json Lines data loader used in fit command\"\"\" def __init__ ( self , filename : str ): self . _filename = filename def __iter__ ( self ): with open ( self . _filename ) as fd : for line in fd : yield ConversationExample ( conversation = json . loads ( line )) train_iterator = JsonLinesIterator ( \"input/train.jsonl\" ) valid_iterator = JsonLinesIterator ( \"input/valid.jsonl\" ) Training Finally, you can start training by calling fit method with train and valid itarators. model.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator ) Fit with model saving Although you can save the model by calling .save_pretrained directly to the model as follows, model.save_pretrained ( \"model\" ) you can also pass the directory to be saved as output_path parameter to fit method. model.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator, output_path = \"model\" ) output_path option allows you to pass save_best_model as a parameter. This option enables to save only the best model based on validation perplexity in output_path . model.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator, output_path = \"model\" , save_best_model = True ) Load trained model Oncde model training is completed, you can load your trained model by .from_pretrained method. model = ConversationModel.from_pretrained ( \"model\" )","title":"Model Training"},{"location":"model_training/#model-training","text":"","title":"Model Training"},{"location":"model_training/#prepare-model","text":"First you need to load GPT-2 pretrained model. The model is easily loaded by using from_pretrained method defined in ConversationModel . from convmodel import ConversationModel model = ConverstationModel . from_pretrained ( \"gpt2\" ) If you want to use GPU to train, device option is for that. # Load model in GPU model = ConverstationModel . from_pretrained ( \"gpt2\" , device = \"cuda\" ) # Load model in CPU model = ConverstationModel . from_pretrained ( \"gpt2\" , device = \"cpu\" ) If you do not specify any values to device , GPU is used if available.","title":"Prepare model"},{"location":"model_training/#training-data","text":"Before training, you also need to prepare training data. convmodel provides ConversationExample class which shows one example of conversation to use in training. You need to prepare iterator objects for train/valid data to provide one ConversationExample object in each step in the loop. from convmodel import ConversationExample train_iterator = [ ConversationExample ( conversation = [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ]), ConversationExample ( conversation = [ \"I am hungry\" , \"How about eating pizza?\" ]), ] valid_iterator = [ ConvesationExample ( conversation = [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ]), ] Although the above example is fine, the data is usually large and difficult to load all the data on memory at the same time. In this case, it might be better to implement iterator class to provide one example in each step in the loop. Following example assumes each data file contains one conversation example in one line. The file format is Json Lines and each line contains a list of string which shows one conversation examples. # Assume that training/valid data is located in under input directory. # Training file: input/train.jsonl $ head -n2 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] # Validation file: input/valid.jsonl $ head -n1 input/valid.jsonl [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] You can implement your own iterator class to load the file and return each conversation example at each time as follows. class JsonLinesIterator : \"\"\"Json Lines data loader used in fit command\"\"\" def __init__ ( self , filename : str ): self . _filename = filename def __iter__ ( self ): with open ( self . _filename ) as fd : for line in fd : yield ConversationExample ( conversation = json . loads ( line )) train_iterator = JsonLinesIterator ( \"input/train.jsonl\" ) valid_iterator = JsonLinesIterator ( \"input/valid.jsonl\" )","title":"Training data"},{"location":"model_training/#training","text":"Finally, you can start training by calling fit method with train and valid itarators. model.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator )","title":"Training"},{"location":"model_training/#fit-with-model-saving","text":"Although you can save the model by calling .save_pretrained directly to the model as follows, model.save_pretrained ( \"model\" ) you can also pass the directory to be saved as output_path parameter to fit method. model.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator, output_path = \"model\" ) output_path option allows you to pass save_best_model as a parameter. This option enables to save only the best model based on validation perplexity in output_path . model.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator, output_path = \"model\" , save_best_model = True )","title":"Fit with model saving"},{"location":"model_training/#load-trained-model","text":"Oncde model training is completed, you can load your trained model by .from_pretrained method. model = ConversationModel.from_pretrained ( \"model\" )","title":"Load trained model"},{"location":"response_generation/","text":"Response Generation Once ConversationModel is trained, the model can be loaded via from_pretrained . Assume your trained model is saved under model directory. Then you can load it as follows. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) After loading your model, generate method generates response from a given context. All the options defined in a transformers generate method can be also available in the generate method. One example below uses top_p and top_k options with do_sample . >>> model . generate ( context = [ \"\u3053\u3093\u306b\u3061\u306f\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) ConversationModelOutput ( responses = [ '\u3053\u3093\u306b\u3061\u306f\u266a' ], context = [ '\u3053\u3093\u306b\u3061\u306f' ])","title":"Response Generation"},{"location":"response_generation/#response-generation","text":"Once ConversationModel is trained, the model can be loaded via from_pretrained . Assume your trained model is saved under model directory. Then you can load it as follows. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) After loading your model, generate method generates response from a given context. All the options defined in a transformers generate method can be also available in the generate method. One example below uses top_p and top_k options with do_sample . >>> model . generate ( context = [ \"\u3053\u3093\u306b\u3061\u306f\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) ConversationModelOutput ( responses = [ '\u3053\u3093\u306b\u3061\u306f\u266a' ], context = [ '\u3053\u3093\u306b\u3061\u306f' ])","title":"Response Generation"}]}