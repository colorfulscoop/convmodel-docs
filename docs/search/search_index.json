{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"convmodel convmodel provides a conversation model based on GPT-2 provided by transformers . Features convmodel utilizes GPT2 model to generate response. convmodel handles multi-turn conversation. convmodel provides an useuful interface to generate a response from a given context. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) >>> model . generate ( context = [ \"\u3053\u3093\u306b\u3061\u306f\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) ConversationModelOutput ( responses = [ '\u3053\u3093\u306b\u3061\u306f\u266a' ], context = [ '\u3053\u3093\u306b\u3061\u306f' ]) position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep> Streamlit interface is available now to try your conversation model as an experimental feature from v0.1.0 Enjoy talking with your conversational AI","title":"Overview"},{"location":"#convmodel","text":"convmodel provides a conversation model based on GPT-2 provided by transformers . Features convmodel utilizes GPT2 model to generate response. convmodel handles multi-turn conversation. convmodel provides an useuful interface to generate a response from a given context. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) >>> model . generate ( context = [ \"\u3053\u3093\u306b\u3061\u306f\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) ConversationModelOutput ( responses = [ '\u3053\u3093\u306b\u3061\u306f\u266a' ], context = [ '\u3053\u3093\u306b\u3061\u306f' ]) position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep> Streamlit interface is available now to try your conversation model as an experimental feature from v0.1.0 Enjoy talking with your conversational AI","title":"convmodel"},{"location":"cli/","text":"CLI (Experimental) Currently convmodel CLI is an experimental feature. To use convmodel CLI, install convmodel with cli option. $ pip install git+https://github.com/colorfulscoop/convmodel [ cli ] fit - Model training This is a simple wrapper interface of ConversationModel.fit method. You can simply run training by json config file via this interface. All you need to do is preparing json config file. A template is prepared under example/fit_config.json . $ cat example/fit_config.json { \"pretrained_model_or_path\" : \"(input your pretrained model path\" , \"output_path\" : \"(input your output path)\" , \"train_file\" : \"(input yout train file)\" , \"valid_file\" : \"(input your valid file)\" , \"device\" : null, \"lr\" : 1e-4, \"warmup_steps\" : 10000 , \"use_amp\" : false, \"epochs\" : 1 , \"accumulation_steps\" : 1 , \"show_progress_bar\" : true, \"log_steps\" : 100 , \"shuffle_buffer_size\" : null, \"batch_size\" : 1 , \"num_workers\" : 0 , \"prefetch_factor\" : 2 , \"seed\" : null, \"deterministic\" : false } At least you need to edit 4 parameters. Parameter Description Example value pretrained_model_or_path Pretrained model path to use gpt2 output_path Path to save your trained model model train_file Path for training data file. The format should be Json Lines. Each line needs to contain a list of string, which are one example of conversation input/train.jsonl valid_file Path for validation data file. Format is the same as train_file . input/valid.jsonl One example of train/valid file is as follows. $ head -n3 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] After preparing config json file, you can start training by fit CLI command. $ python -m convmodel fit --config example/fit_config.json After completing training, you can load the trained model from output_path for ConversationModel . >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) run_streamlit - Conversation test interface convmodel CLI provides streamlit interface to test conversation of your model. # Default server address and port will be used $ python -m convmodel.cli run_streamlit # You can set server port via --server.port option $ python -m convmodel.cli run_streamlit --server.port 8080 # You can set server address and port via --server.address $ python -m convmodel.cli run_streamlit --server.port 8080 --server.address 0 .0.0.0 # You can check all options by --help $ python -m convmodel.cli run_streamlit --help As default, you can access UI via http://localhost:8501/ .","title":"CLI (Experimental)"},{"location":"cli/#cli-experimental","text":"Currently convmodel CLI is an experimental feature. To use convmodel CLI, install convmodel with cli option. $ pip install git+https://github.com/colorfulscoop/convmodel [ cli ]","title":"CLI (Experimental)"},{"location":"cli/#fit-model-training","text":"This is a simple wrapper interface of ConversationModel.fit method. You can simply run training by json config file via this interface. All you need to do is preparing json config file. A template is prepared under example/fit_config.json . $ cat example/fit_config.json { \"pretrained_model_or_path\" : \"(input your pretrained model path\" , \"output_path\" : \"(input your output path)\" , \"train_file\" : \"(input yout train file)\" , \"valid_file\" : \"(input your valid file)\" , \"device\" : null, \"lr\" : 1e-4, \"warmup_steps\" : 10000 , \"use_amp\" : false, \"epochs\" : 1 , \"accumulation_steps\" : 1 , \"show_progress_bar\" : true, \"log_steps\" : 100 , \"shuffle_buffer_size\" : null, \"batch_size\" : 1 , \"num_workers\" : 0 , \"prefetch_factor\" : 2 , \"seed\" : null, \"deterministic\" : false } At least you need to edit 4 parameters. Parameter Description Example value pretrained_model_or_path Pretrained model path to use gpt2 output_path Path to save your trained model model train_file Path for training data file. The format should be Json Lines. Each line needs to contain a list of string, which are one example of conversation input/train.jsonl valid_file Path for validation data file. Format is the same as train_file . input/valid.jsonl One example of train/valid file is as follows. $ head -n3 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] After preparing config json file, you can start training by fit CLI command. $ python -m convmodel fit --config example/fit_config.json After completing training, you can load the trained model from output_path for ConversationModel . >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" )","title":"fit - Model training"},{"location":"cli/#run_streamlit-conversation-test-interface","text":"convmodel CLI provides streamlit interface to test conversation of your model. # Default server address and port will be used $ python -m convmodel.cli run_streamlit # You can set server port via --server.port option $ python -m convmodel.cli run_streamlit --server.port 8080 # You can set server address and port via --server.address $ python -m convmodel.cli run_streamlit --server.port 8080 --server.address 0 .0.0.0 # You can check all options by --help $ python -m convmodel.cli run_streamlit --help As default, you can access UI via http://localhost:8501/ .","title":"run_streamlit - Conversation test interface"},{"location":"install/","text":"Install First, install Python >= 3.8. Install PyTorch Then install PyTorch >= 1.8,<=1.9. Please refer to official document to find out correct installation for your environment. Some examples of installtion are as follows. Install in Docker container without GPU $ docker container run -w /work -v $( pwd ) :/work --rm -it python:3.8.6-slim-buster bash $ pip install torch == 1 .8.1 Install in Docker container enabling GPU and CUDA 11.1 Assume that CUDA 11.1 is installed in your environment. $ docker container run --gpus all --ipc = host --rm -it -v $( pwd ) :/work -w /work nvidia/cuda:11.1-devel-ubuntu20.04 bash Note: --ipc option is required because share memory would not be enough because DataLoader multiprocess requires them. Refer to the pytorch discussion for more details. $ apt update && apt install -y python3 python3-pip git Install PyTorch which corresponds to your environment by following the installation guide . For example, in CUDA 11.1 environment, PyTorch can be installed as follows. $ pip3 install torch == 1 .8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html Install convmodel Finally, install convmodel: $ pip install git+https://github.com/colorfulscoop/convmodel","title":"Install"},{"location":"install/#install","text":"First, install Python >= 3.8.","title":"Install"},{"location":"install/#install-pytorch","text":"Then install PyTorch >= 1.8,<=1.9. Please refer to official document to find out correct installation for your environment. Some examples of installtion are as follows.","title":"Install PyTorch"},{"location":"install/#install-in-docker-container-without-gpu","text":"$ docker container run -w /work -v $( pwd ) :/work --rm -it python:3.8.6-slim-buster bash $ pip install torch == 1 .8.1","title":"Install in Docker container without GPU"},{"location":"install/#install-in-docker-container-enabling-gpu-and-cuda-111","text":"Assume that CUDA 11.1 is installed in your environment. $ docker container run --gpus all --ipc = host --rm -it -v $( pwd ) :/work -w /work nvidia/cuda:11.1-devel-ubuntu20.04 bash Note: --ipc option is required because share memory would not be enough because DataLoader multiprocess requires them. Refer to the pytorch discussion for more details. $ apt update && apt install -y python3 python3-pip git Install PyTorch which corresponds to your environment by following the installation guide . For example, in CUDA 11.1 environment, PyTorch can be installed as follows. $ pip3 install torch == 1 .8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html","title":"Install in Docker container enabling GPU and CUDA 11.1"},{"location":"install/#install-convmodel","text":"Finally, install convmodel: $ pip install git+https://github.com/colorfulscoop/convmodel","title":"Install convmodel"},{"location":"model_architecture_overview/","text":"Model Architecture Overview convmodel provides ConversationModel class. ConversationModel class adopts GPT2LMHeadModel architecture provided by transformers library. Although, in a initializer of ConversationModel , ConversationTokenizer is automatically initialized, let us first directly initialize ConversationTokenizer to see it encodes a given context to input to the model. Assume that ConversationTokenizer gets a context [\"Hello\", \"How are you\"] . Then ConversationTokenizer encodes it as follows. >>> from convmodel import ConversationTokenizer >>> tokenizer = ConversationTokenizer . from_pretrained ( \"gpt2\" ) >>> context = [ \"Hello\" , \"How are you\" ] >>> tokenizer ( context ) { 'input_ids' : [ 50256 , 15496 , 50256 , 2437 , 389 , 345 , 50256 ], 'token_type_ids' : [ 0 , 0 , 1 , 1 , 1 , 1 , 0 ], 'attention_mask' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 ]} position 0 1 2 3 4 5 6 word \\<sep> Hello \\<sep> How are you \\<sep> input_ids 50256 15496 50256 2437 389 345 50256 token_type_ids 0 0 1 1 1 1 0 attention_mask 1 1 1 1 1 1 1 Note: if a tokenizer does not assign a value to sep_token_id , it is automatically set with sep_token of <sep> . When initializing ConversationModel , ConversationTokenizer is automatically initialized inside. ConversationModel implements generate method. In generate method, an input context is first encoded as above. Then the encoded tensors are forwardded by the model to predict following tokens until <sep> token appears Note: Here we assume that model directory contains a trained conversation model which was fine-tuned from gpt2 model. We will see how to train our own conversation model later. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) >>> model . generate ( context , do_sample = True , top_p = 0.95 ) position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep>","title":"Model Architecture"},{"location":"model_architecture_overview/#model-architecture-overview","text":"convmodel provides ConversationModel class. ConversationModel class adopts GPT2LMHeadModel architecture provided by transformers library. Although, in a initializer of ConversationModel , ConversationTokenizer is automatically initialized, let us first directly initialize ConversationTokenizer to see it encodes a given context to input to the model. Assume that ConversationTokenizer gets a context [\"Hello\", \"How are you\"] . Then ConversationTokenizer encodes it as follows. >>> from convmodel import ConversationTokenizer >>> tokenizer = ConversationTokenizer . from_pretrained ( \"gpt2\" ) >>> context = [ \"Hello\" , \"How are you\" ] >>> tokenizer ( context ) { 'input_ids' : [ 50256 , 15496 , 50256 , 2437 , 389 , 345 , 50256 ], 'token_type_ids' : [ 0 , 0 , 1 , 1 , 1 , 1 , 0 ], 'attention_mask' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 ]} position 0 1 2 3 4 5 6 word \\<sep> Hello \\<sep> How are you \\<sep> input_ids 50256 15496 50256 2437 389 345 50256 token_type_ids 0 0 1 1 1 1 0 attention_mask 1 1 1 1 1 1 1 Note: if a tokenizer does not assign a value to sep_token_id , it is automatically set with sep_token of <sep> . When initializing ConversationModel , ConversationTokenizer is automatically initialized inside. ConversationModel implements generate method. In generate method, an input context is first encoded as above. Then the encoded tensors are forwardded by the model to predict following tokens until <sep> token appears Note: Here we assume that model directory contains a trained conversation model which was fine-tuned from gpt2 model. We will see how to train our own conversation model later. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) >>> model . generate ( context , do_sample = True , top_p = 0.95 ) position 0 1 2 3 4 5 6 7 8 9 word \\<sep> Hello \\<sep> How are you \\<sep> Good thank you input_ids 50256 15496 50256 2437 389 345 50256 10248 5875 345 token_type_ids 0 0 1 1 1 1 0 0 0 0 attention_mask 1 1 1 1 1 1 1 1 1 1 \u2193 \u2193 \u2193 \u2193 generated word - - - - - - Good thank you \\<sep>","title":"Model Architecture Overview"},{"location":"model_training/","text":"Model Training Prepare model First you need to load GPT-2 pretrained model. The model is easily loaded by using from_pretrained method defined in ConversationModel . from convmodel import ConversationModel model = ConverstationModel . from_pretrained ( \"gpt-2\" ) If you want to use GPU to train, device option is for that. # Load model in GPU model = ConverstationModel . from_pretrained ( \"gpt-2\" , device = \"cuda\" ) # Load model in CPU model = ConverstationModel . from_pretrained ( \"gpt-2\" , device = \"cpu\" ) If you do not specify any values to device , GPU is used if available. Training data Before training, you also need to prepare training data. convmodel provides ConversationExample class which shows one example of conversation to use in training. You need to prepare iterator objects for train/valid data to provide one ConversationExample object in each step in the loop. from convmodel import ConversationExample train_iterator = [ ConversationExample ( conversation = [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ]), ConversationExample ( conversation = [ \"I am hungry\" , \"How about eating pizza?\" ]), ] valid_iterator = [ ConvesationExample ( conversation = [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ]), ] Although the above example is fine, the data is usually large and difficult to load all the data on memory at the same time. In this case, it might be better to implement iterator class to provide one example in each step in the loop. Following example assumes each data file contains one conversation example in one line. The file format is Json Lines and each line contains a list of string which shows one conversation examples. # Assume that training/valid data is located in under input directory. # Training file: input/train.jsonl $ head -n2 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] # Validation file: input/valid.jsonl $ head -n1 input/valid.jsonl [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] You can implement your own iterator class to load the file and return each conversation example at each time as follows. class JsonLinesIterator : \"\"\"Json Lines data loader used in fit command\"\"\" def __init__ ( self , filename : str ): self . _filename = filename def __iter__ ( self ): with open ( self . _filename ) as fd : for line in fd : yield ConversationExample ( conversation = json . loads ( line )) train_iterator = JsonLinesIterator ( \"input/train.jsonl\" ) valid_iterator = JsonLinesIterator ( \"input/valid.jsonl\" ) Training Finally, you can start training by calling fit method with train/valid itarators. model = ConversationModel.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator )","title":"Model Training"},{"location":"model_training/#model-training","text":"","title":"Model Training"},{"location":"model_training/#prepare-model","text":"First you need to load GPT-2 pretrained model. The model is easily loaded by using from_pretrained method defined in ConversationModel . from convmodel import ConversationModel model = ConverstationModel . from_pretrained ( \"gpt-2\" ) If you want to use GPU to train, device option is for that. # Load model in GPU model = ConverstationModel . from_pretrained ( \"gpt-2\" , device = \"cuda\" ) # Load model in CPU model = ConverstationModel . from_pretrained ( \"gpt-2\" , device = \"cpu\" ) If you do not specify any values to device , GPU is used if available.","title":"Prepare model"},{"location":"model_training/#training-data","text":"Before training, you also need to prepare training data. convmodel provides ConversationExample class which shows one example of conversation to use in training. You need to prepare iterator objects for train/valid data to provide one ConversationExample object in each step in the loop. from convmodel import ConversationExample train_iterator = [ ConversationExample ( conversation = [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ]), ConversationExample ( conversation = [ \"I am hungry\" , \"How about eating pizza?\" ]), ] valid_iterator = [ ConvesationExample ( conversation = [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ]), ] Although the above example is fine, the data is usually large and difficult to load all the data on memory at the same time. In this case, it might be better to implement iterator class to provide one example in each step in the loop. Following example assumes each data file contains one conversation example in one line. The file format is Json Lines and each line contains a list of string which shows one conversation examples. # Assume that training/valid data is located in under input directory. # Training file: input/train.jsonl $ head -n2 input/train.jsonl [ \"Hello\" , \"Hi, how are you?\" , \"Good, thank you, how about you?\" , \"Good, thanks!\" ] [ \"I am hungry\" , \"How about eating pizza?\" ] # Validation file: input/valid.jsonl $ head -n1 input/valid.jsonl [ \"Tired...\" , \"Let's have a break!\" , \"Nice idea!\" ] You can implement your own iterator class to load the file and return each conversation example at each time as follows. class JsonLinesIterator : \"\"\"Json Lines data loader used in fit command\"\"\" def __init__ ( self , filename : str ): self . _filename = filename def __iter__ ( self ): with open ( self . _filename ) as fd : for line in fd : yield ConversationExample ( conversation = json . loads ( line )) train_iterator = JsonLinesIterator ( \"input/train.jsonl\" ) valid_iterator = JsonLinesIterator ( \"input/valid.jsonl\" )","title":"Training data"},{"location":"model_training/#training","text":"Finally, you can start training by calling fit method with train/valid itarators. model = ConversationModel.fit ( train_iterator = train_iterator, valid_iterator = valid_iterator )","title":"Training"},{"location":"response_generation/","text":"Response Generation Once ConversationModel is trained, the model can be loaded via from_pretrained . Assume your trained model is saved under model directory. Then you can load it as follows. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) After loading your model, generate method generates response from a given context. All the options defined in a transformers generate method can be also available in the generate method. One example below uses top_p and top_k options with do_sample . >>> model . generate ( context = [ \"\u3053\u3093\u306b\u3061\u306f\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) ConversationModelOutput ( responses = [ '\u3053\u3093\u306b\u3061\u306f\u266a' ], context = [ '\u3053\u3093\u306b\u3061\u306f' ])","title":"Response Generation"},{"location":"response_generation/#response-generation","text":"Once ConversationModel is trained, the model can be loaded via from_pretrained . Assume your trained model is saved under model directory. Then you can load it as follows. >>> from convmodel import ConversationModel >>> model = ConversationModel . from_pretrained ( \"model\" ) After loading your model, generate method generates response from a given context. All the options defined in a transformers generate method can be also available in the generate method. One example below uses top_p and top_k options with do_sample . >>> model . generate ( context = [ \"\u3053\u3093\u306b\u3061\u306f\" ], do_sample = True , top_p = 0.95 , top_k = 50 ) ConversationModelOutput ( responses = [ '\u3053\u3093\u306b\u3061\u306f\u266a' ], context = [ '\u3053\u3093\u306b\u3061\u306f' ])","title":"Response Generation"}]}